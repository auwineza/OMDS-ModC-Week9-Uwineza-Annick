{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df201ddb",
   "metadata": {},
   "source": [
    "# Week 9 — Gradient Boost (Scikit‑learn)\n",
    "*Generated: 2025-11-02 04:58:46*\n",
    "\n",
    "This notebook applies **Gradient Boosting** concepts to your Integrated Capstone dataset. It covers:\n",
    "- Gradient boosting for **regression or classification** (auto‑detected).\n",
    "- Key hyperparameters: **learning rate**, **number of estimators**, **tree depth/leaves**, **subsample**, **max_features**.\n",
    "- Regularization: **min_samples_leaf**, **max_depth / max_leaf_nodes**, **subsample**, **max_features**, **L2** (via Histogram‑based GBDT).\n",
    "- **Cross‑validation**, **early stopping** (for `HistGradientBoosting*`), and **hyperparameter tuning**.\n",
    "- Diagnostics: performance metrics, curves, and **permutation importance**.\n",
    "\n",
    "> **Tip:** If your capstone CSV is not ready, enable the demo toggle below to run with a built‑in dataset so the notebook executes end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup ===\n",
    "# Toggle to use a demo dataset if your CSV isn't ready yet.\n",
    "USE_DEMO_DATA = True  # set to False when you have your own CSV\n",
    "\n",
    "# Filepath to your project CSV (only used if USE_DEMO_DATA = False)\n",
    "CSV_PATH = \"path/to/your_capstone.csv\"\n",
    "\n",
    "# Name of your target column in the CSV (used if not using demo data)\n",
    "TARGET_COL = \"YourTargetColumnName\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# CV folds\n",
    "CV_FOLDS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683c836",
   "metadata": {},
   "source": [
    "## 1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_DEMO_DATA:\n",
    "    # Demo option detects classification vs regression automatically by switching datasets\n",
    "    from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "\n",
    "    DEMO_TASK = \"classification\"  # change to \"regression\" to see regression flow\n",
    "\n",
    "    if DEMO_TASK == \"classification\":\n",
    "        ds = load_breast_cancer(as_frame=True)\n",
    "        df = ds.frame.copy()\n",
    "        TARGET_COL = \"target\"\n",
    "    else:\n",
    "        ds = fetch_california_housing(as_frame=True)\n",
    "        df = ds.frame.copy()\n",
    "        TARGET_COL = \"MedHouseVal\"\n",
    "else:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd0b0a",
   "metadata": {},
   "source": [
    "## 2) Quick EDA & Target Type Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Heuristic to detect classification vs regression\n",
    "if pd.api.types.is_bool_dtype(y) or (y.nunique() <= 10 and set(y.unique()).issubset({0,1})):\n",
    "    TASK = \"classification\"\n",
    "elif y.dtype.kind in {\"i\",\"u\"} and y.nunique() <= 10:\n",
    "    TASK = \"classification\"\n",
    "else:\n",
    "    TASK = \"regression\"\n",
    "\n",
    "print(\"Detected task:\", TASK, \"| Target:\", TARGET_COL, \"| Unique target values:\", y.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c5971",
   "metadata": {},
   "source": [
    "## 3) Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df83dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE,\n",
    "    stratify=y if TASK == \"classification\" else None\n",
    ")\n",
    "print(X_train.shape, X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c8f89",
   "metadata": {},
   "source": [
    "## 4) Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column splits\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "print(f\"Numeric cols: {len(num_cols)} | Categorical cols: {len(cat_cols)}\")\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))  # tree models don't need scaling; kept to be safe for mixed pipelines\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1067b",
   "metadata": {},
   "source": [
    "## 5) Baseline Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TASK == \"regression\":\n",
    "    base_est = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "else:\n",
    "    base_est = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "pipe_base = Pipeline([(\"prep\", preprocess), (\"gb\", base_est)])\n",
    "pipe_base.fit(X_train, y_train)\n",
    "\n",
    "def evaluate(pipe, X_tr, y_tr, X_te, y_te, task):\n",
    "    pred_tr = pipe.predict(X_tr)\n",
    "    pred_te = pipe.predict(X_te)\n",
    "    if task == \"classification\":\n",
    "        metrics = {\n",
    "            \"accuracy_tr\": accuracy_score(y_tr, pred_tr),\n",
    "            \"accuracy_te\": accuracy_score(y_te, pred_te),\n",
    "            \"f1_te\": f1_score(y_te, pred_te, average=\"binary\" if y_te.nunique()==2 else \"macro\"),\n",
    "        }\n",
    "        # AUC if possible\n",
    "        try:\n",
    "            proba = pipe.predict_proba(X_te)[:,1]\n",
    "            metrics[\"roc_auc_te\"] = roc_auc_score(y_te, proba)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        metrics = {\n",
    "            \"rmse_tr\": mean_squared_error(y_tr, pred_tr, squared=False),\n",
    "            \"rmse_te\": mean_squared_error(y_te, pred_te, squared=False),\n",
    "            \"mae_te\": mean_absolute_error(y_te, pred_te),\n",
    "            \"r2_te\": r2_score(y_te, pred_te),\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "metrics_base = evaluate(pipe_base, X_train, y_train, X_valid, y_valid, TASK)\n",
    "metrics_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33fab8",
   "metadata": {},
   "source": [
    "## 6) Learning Rate × Number of Estimators (Bias–Variance Trade‑off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5710fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrs = [0.02, 0.05, 0.1, 0.2]\n",
    "nest_opts = [100, 300, 600]\n",
    "\n",
    "results = []\n",
    "for lr in lrs:\n",
    "    for ne in nest_opts:\n",
    "        if TASK == \"regression\":\n",
    "            est = GradientBoostingRegressor(learning_rate=lr, n_estimators=ne, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            est = GradientBoostingClassifier(learning_rate=lr, n_estimators=ne, random_state=RANDOM_STATE)\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"gb\", est)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        m = evaluate(pipe, X_train, y_train, X_valid, y_valid, TASK)\n",
    "        m[\"learning_rate\"] = lr\n",
    "        m[\"n_estimators\"] = ne\n",
    "        results.append(m)\n",
    "\n",
    "df_lr = pd.DataFrame(results)\n",
    "display(df_lr.sort_values(by=list(df_lr.columns)[:1]).reset_index(drop=True))\n",
    "\n",
    "# Simple visualization: plot score vs n_estimators for each LR\n",
    "plt.figure(figsize=(8,5))\n",
    "for lr in lrs:\n",
    "    sub = df_lr[df_lr[\"learning_rate\"] == lr]\n",
    "    x = sub[\"n_estimators\"].values\n",
    "    if TASK == \"classification\":\n",
    "        y = sub[\"accuracy_te\"].values\n",
    "        plt.plot(x, y, marker=\"o\", label=f\"lr={lr}\")\n",
    "        plt.ylabel(\"Accuracy (valid)\")\n",
    "    else:\n",
    "        y = sub[\"rmse_te\"].values\n",
    "        plt.plot(x, y, marker=\"o\", label=f\"lr={lr}\")\n",
    "        plt.ylabel(\"RMSE (valid)\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.title(\"Learning Rate vs n_estimators (Validation)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91ef93",
   "metadata": {},
   "source": [
    "## 7) Tree Depth & Leaves (Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depths = [2, 3, 4, 6]\n",
    "min_leaves = [1, 5, 10]\n",
    "\n",
    "rows = []\n",
    "for d in depths:\n",
    "    for ml in min_leaves:\n",
    "        if TASK == \"regression\":\n",
    "            est = GradientBoostingRegressor(max_depth=d, min_samples_leaf=ml, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            est = GradientBoostingClassifier(max_depth=d, min_samples_leaf=ml, random_state=RANDOM_STATE)\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"gb\", est)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        m = evaluate(pipe, X_train, y_train, X_valid, y_valid, TASK)\n",
    "        m.update({\"max_depth\": d, \"min_samples_leaf\": ml})\n",
    "        rows.append(m)\n",
    "\n",
    "df_depth = pd.DataFrame(rows)\n",
    "display(df_depth)\n",
    "\n",
    "# Plot depth vs metric at best min_samples_leaf\n",
    "if TASK == \"classification\":\n",
    "    best_ml = df_depth.sort_values(\"accuracy_te\", ascending=False).iloc[0][\"min_samples_leaf\"]\n",
    "    sub = df_depth[df_depth[\"min_samples_leaf\"] == best_ml]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(sub[\"max_depth\"], sub[\"accuracy_te\"], marker=\"o\")\n",
    "    plt.xlabel(\"max_depth\")\n",
    "    plt.ylabel(\"Accuracy (valid)\")\n",
    "    plt.title(f\"Depth vs Accuracy (min_samples_leaf={int(best_ml)})\")\n",
    "    plt.show()\n",
    "else:\n",
    "    best_ml = df_depth.sort_values(\"rmse_te\", ascending=True).iloc[0][\"min_samples_leaf\"]\n",
    "    sub = df_depth[df_depth[\"min_samples_leaf\"] == best_ml]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(sub[\"max_depth\"], sub[\"rmse_te\"], marker=\"o\")\n",
    "    plt.xlabel(\"max_depth\")\n",
    "    plt.ylabel(\"RMSE (valid)\")\n",
    "    plt.title(f\"Depth vs RMSE (min_samples_leaf={int(best_ml)})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb5c18",
   "metadata": {},
   "source": [
    "## 8) Stochastic Gradient Boosting (subsample, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subsamps = [0.6, 0.8, 1.0]\n",
    "max_feats = [None, 0.5, 0.8]\n",
    "\n",
    "rows = []\n",
    "for ss in subsamps:\n",
    "    for mf in max_feats:\n",
    "        if TASK == \"regression\":\n",
    "            est = GradientBoostingRegressor(subsample=ss, max_features=mf, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            est = GradientBoostingClassifier(subsample=ss, max_features=mf, random_state=RANDOM_STATE)\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"gb\", est)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        m = evaluate(pipe, X_train, y_train, X_valid, y_valid, TASK)\n",
    "        m.update({\"subsample\": ss, \"max_features\": mf})\n",
    "        rows.append(m)\n",
    "\n",
    "df_stoch = pd.DataFrame(rows)\n",
    "display(df_stoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255357b6",
   "metadata": {},
   "source": [
    "## 9) Histogram-based GBDT with Early Stopping & L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TASK == \"regression\":\n",
    "    est = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=None,            # use max_leaf_nodes instead for HGBDT\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.0,     # try >0 for more regularization\n",
    "        early_stopping=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "else:\n",
    "    est = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=None,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.0,\n",
    "        early_stopping=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "pipe_hgb = Pipeline([(\"prep\", preprocess), (\"hgb\", est)])\n",
    "pipe_hgb.fit(X_train, y_train)\n",
    "metrics_hgb = evaluate(pipe_hgb, X_train, y_train, X_valid, y_valid, TASK)\n",
    "metrics_hgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0339264",
   "metadata": {},
   "source": [
    "## 10) Hyperparameter Tuning (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TASK == \"regression\":\n",
    "    model = HistGradientBoostingRegressor(random_state=RANDOM_STATE, early_stopping=True)\n",
    "    param_dist = {\n",
    "        \"hgb__learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"hgb__max_leaf_nodes\": randint(15, 63),\n",
    "        \"hgb__min_samples_leaf\": randint(5, 60),\n",
    "        \"hgb__l2_regularization\": loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "else:\n",
    "    model = HistGradientBoostingClassifier(random_state=RANDOM_STATE, early_stopping=True)\n",
    "    param_dist = {\n",
    "        \"hgb__learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"hgb__max_leaf_nodes\": randint(15, 63),\n",
    "        \"hgb__min_samples_leaf\": randint(5, 60),\n",
    "        \"hgb__l2_regularization\": loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "\n",
    "pipe = Pipeline([(\"prep\", preprocess), (\"hgb\", model)])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE) if TASK == \"classification\" else KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "scoring = \"roc_auc\" if TASK == \"classification\" else \"neg_root_mean_squared_error\"\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV score:\", search.best_score_)\n",
    "\n",
    "best_pipe = search.best_estimator_\n",
    "tuned_metrics = evaluate(best_pipe, X_train, y_train, X_valid, y_valid, TASK)\n",
    "tuned_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6291e8",
   "metadata": {},
   "source": [
    "## 11) Permutation Importance (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4223a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perm = permutation_importance(best_pipe, X_valid, y_valid, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "imp_idx = perm.importances_mean.argsort()[::-1]\n",
    "feature_names = list(best_pipe.named_steps[\"prep\"].get_feature_names_out())\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": np.array(feature_names)[imp_idx],\n",
    "    \"importance_mean\": perm.importances_mean[imp_idx],\n",
    "    \"importance_std\": perm.importances_std[imp_idx]\n",
    "})\n",
    "display(imp_df.head(25))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "topn = min(20, len(imp_df))\n",
    "plt.barh(imp_df[\"feature\"][:topn][::-1], imp_df[\"importance_mean\"][:topn][::-1])\n",
    "plt.xlabel(\"Permutation Importance (mean decrease in score)\")\n",
    "plt.title(\"Top Features (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e29e52",
   "metadata": {},
   "source": [
    "## 12) Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TASK == \"regression\":\n",
    "    y_pred = best_pipe.predict(X_valid)\n",
    "    residuals = y_valid - y_pred\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residual (y - ŷ)\")\n",
    "    plt.title(\"Residuals vs Predicted (Validation)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # Calibration-ish check via predicted probs histogram\n",
    "    try:\n",
    "        proba = best_pipe.predict_proba(X_valid)[:,1]\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.hist(proba, bins=20)\n",
    "        plt.xlabel(\"Predicted probability (positive class)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Predicted Probabilities — Validation\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute probabilities:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3b450",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Results Summary (Fill These In)\n",
    "- **Task:** {{classification/regression}} on **`{TARGET_COL}`**.\n",
    "- **Baseline GradientBoosting:** (report metrics from `metrics_base`).\n",
    "- **HistGradientBoosting (early stopping):** (report `metrics_hgb`).\n",
    "- **Tuned model (RandomizedSearchCV):** (report `tuned_metrics` and best hyperparameters).\n",
    "\n",
    "**What helped avoid overfitting?**  \n",
    "- Appropriate **tree depth / min_samples_leaf** limited complexity.  \n",
    "- **Subsample** and **max_features** (stochastic GB) added randomness → less variance.  \n",
    "- **Early stopping** (HGBDT) stopped when validation score stopped improving.  \n",
    "- **Cross‑validation** guided robust hyperparameter selection.  \n",
    "- **L2 regularization** (HGBDT) controlled leaf weights.\n",
    "\n",
    "**What metrics did you use and why?**  \n",
    "- **Classification:** Accuracy / F1; **ROC AUC** to capture ranking quality under class imbalance.  \n",
    "- **Regression:** **RMSE** (penalizes large errors), **MAE** (robust), **R²** (variance explained).\n",
    "\n",
    "**Expected vs. unexpected findings:**  \n",
    "- Note any surprising feature importances, diminishing returns from very small learning rates, or overfitting when trees are too deep.\n",
    "\n",
    "**How did EDA help?**  \n",
    "- Informed which variables to treat as categorical, guided imputation strategies, and highlighted outliers influencing GB sensitivity.\n",
    "\n",
    "**Citations you used this week (add APA references in your write‑up):**  \n",
    "- Scikit‑learn user guide on Gradient Boosting and Histogram‑based Gradient Boosting.\n",
    "- Any additional articles, docs, or tutorials you consulted.\n",
    "\n",
    "> _Reminder_: In your Week 12 summary, highlight where you went **deep** (e.g., Week 9: Gradient Boost) per the rubric.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
